ggplot(data, aes(x = y)) +
geom_density()
ggplot(data, aes(x = x, y = y)) +
geom_point()
# From Scratch - Create linear log likelihood function ##
linear_lik <- function(y, X, theta) {
n      <- nrow(X)
k      <- ncol(X)
beta   <- theta[1:k]
sigma2 <- theta[k+1]^2
e      <- y - X%*%beta
logl   <- -.5*n*log(2*pi)-.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return(-logl)
}
# Run MLE using linear likelihood and data
linear_MLE <- optim(fn=linear_lik, y =data$y, X =cbind(1, data$x), par=c(1,1,1), hessian=TRUE, method="BFGS")
linear_MLE$par
stargazer(linear_MLE)
linear_MLE$par
summary(linear_MLE)
stargazer(linear_MLE$par)
## GLM built in function ##
GLM <- glm(data$y~data$x, family = gaussian)
summary(GLM)
stargazer(summary(GLM))
## GLM built in function ##
GLM <- coefficients(glm(data$y~data$x, family = gaussian))
stargazer(GLM)
## OLS LM
lm(data$y~data$x)
## OLS LM
LM <- lm(data$y~data$x)
stargazer(LM)
## OLS LM
LM <- coefficients(lm(data$y~data$x))
stargazer(LM)
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("stargazer", "broom"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
linear_lik <- function(y, X, theta) {
n      <- nrow(X)
k      <- ncol(X)
beta   <- theta[1:k]
sigma2 <- theta[k+1]^2
e      <- y - X%*%beta
logl   <- -.5*n*log(2*pi)-.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return(-logl)
}
# Run MLE using linear likelihood and data
linear_MLE <- optim(fn=linear_lik, y =data$y, X =cbind(1, data$x), par=c(1,1,1), hessian=TRUE, method="BFGS")
broom::tidy(linear_MLE)
linear_MLE$hessian
summary(linear_MLE)
linear_MLE$par
## GLM built in function ##
GLM <- coefficients(glm(data$y~data$x, family = gaussian))
broom::tidy(GLM, conf.int = TRUE)
## GLM built in function ##
GLM <- coefficients(glm(data$y~data$x, family = BFGS))
stargazer(reg1,
type = "latex",
title = "Linear Likelihood Model using BFGS",
covariate.labels = "x", #label explanatory vars
dep.var.labels = "y") #label predicted var
stargazer(linear_MLE,
type = "latex",
title = "Linear Likelihood Model using BFGS",
covariate.labels = "x", #label explanatory vars
dep.var.labels = "y") #label predicted var
stargazer(linear_MLE$par,
type = "latex",
title = "Linear Likelihood Model using BFGS",
covariate.labels = "x", #label explanatory vars
dep.var.labels = "y") #label predicted var
tab = data.frame(
'(Intercept)' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3],
)
linear_MLE_par <- linear_MLE$par
tab = data.frame(
'(Intercept)' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3],
)
tab = data.frame(
'(Intercept)' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3]
)
stargazer(tab)
library(xtable)
xtable(tab)
xtable(linear_MLE_par)
tab = data.frame(
'Intercept' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3]
)
xtable(tab)
table = data.frame(
'Intercept' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3]
)
table
stargazer(GLM)
broom::tidy(GLM, conf.int = TRUE)
## GLM built in function ##
GLM <- glm(data$y~data$x, family = gaussian)
broom::tidy(GLM, conf.int = TRUE)
View(GLM)
stargazer(
GLM,
type = "latex",
keep = "(Intercept)",
omit.summary.stat = TRUE
)
stargazer(
GLM,
type = "latex",
keep = "(Intercept)"
)
stargazer(
GLM
)
## GLM built in function ##
GLM <- coefficients(glm(data$y~data$x, family = gaussian))
stargazer(GLM)
##  From Scratch - Create linear log likelihood function ##
linear_lik <- function(y, X, theta) { #output, input, theta = vector of unknown parms
n      <- nrow(X) #size of sample
k      <- ncol(X) # number of parameters to estimate
beta   <- theta[1:k] # vector of coefficients
sigma2 <- theta[k+1]^2 # variance
e      <- y - X%*%beta #errors with mean of Xbeta (as normal distribution)
logl   <- -.5*n*log(2*pi)-.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return(-logl) #as optim function finds min reverse log likelihood
}
# Run MLE using linear likelihood and data
linear_MLE <- optim(fn=linear_lik,  #using scratch function
y =data$y,  #outcome var
X =cbind(1, data$x),  #input var plus 1's for intercept
par=c(1,1,1),  #parameter starting points
hessian=TRUE, #find standard errors
method="BFGS")
linear_MLE_par <- linear_MLE$par
table = data.frame(
'Intercept' = linear_MLE_par[1],
'x' = linear_MLE_par[2],
'Sigma Squared' = linear_MLE_par[3]
)
table
xtable(table)
stargazer(table)
xtable(linear_MLE$par)
xtable(table)
# Stargazer output
stargazer(LM, GLM,
type = "latex",
column.labels = c("OLS LM", "GLS"),
model.names = FALSE,
omit.stat = c(
"f", "ser", "rsq", "adj.rsq",
"deviance", "null.deviance"
),
title = "Linear OLS and Loglikelihood Regression Results"
)
## GLM built in function ##
GLM <- glm(data$y~data$x, family = gaussian))
## GLM built in function ##
GLM <- glm(data$y~data$x, family = gaussian)
## OLS LM
LM <- lm(data$y~data$x)
# Stargazer output
stargazer(LM, GLM,
type = "latex",
column.labels = c("OLS LM", "GLS"),
model.names = FALSE,
omit.stat = c(
"f", "ser", "rsq", "adj.rsq",
"deviance", "null.deviance"
),
title = "Linear OLS and Loglikelihood Regression Results"
)
# Stargazer output
stargazer(LM, GLM,
type = "latex",
column.labels = c("OLS LM", "GLS"),
model.names = FALSE,
omit.stat = c(
"f", "ser", "rsq", "adj.rsq",
"deviance", "null.deviance"
),
title = "Linear OLS and Loglikelihood Regression Results"
)
# Stargazer output
stargazer(LM, GLM,
type = "latex",
column.labels = c("OLS LM", "GLS"),
model.names = FALSE,
omit.stat = c(
"f", "ser", "rsq", "adj.rsq",
"ll", "aic", "bic",
"deviance", "null.deviance"
),
title = "Linear OLS and Loglikelihood Regression Results"
)
## OLS LM
LM <- coefficients(lm(data$y~data$x))
# Stargazer output
stargazer(LM)
## OLS LM
LM <- lm(data$y~data$x
## OLS LM
LM <- lm(data$y~data$x)
## OLS LM
LM <- lm(data$y~data$x)
# Stargazer output
stargazer(LM)
View(LM)
# GLS Gaussian
GLM_gaussian <- glm(data$y~data$x, family = "gaussian")
stargazer(LM,
type = "latex",
keep = "(Intercept)")
stargazer(LM,
type = "latex",
title = "OLS Linear Regession lm")
# GLS Gaussian
GLM_gaussian <- glm(data$y~data$x, family = "gaussian")
# Stargazer output
stargazer(GLM_gaussian,
type = "latex",
title = "GLM gaussian")
View(linear_MLE)
View(linear_MLE)
xtable(linear_MLE$hessian)
vcov <- solve(linear_MLE$hessian)
se <-sqrt(diag(vcov))
se
H <- linear_MLE$hessian
vcov <- solve(-H)
se <-sqrt(diag(vcov))
se
H <- linear_MLE$hessian
vcov <- solve(-H)
se <-sqrt(diag(vcov))
se
vcov <- solve(H)
se <-sqrt(diag(vcov))
se
linear_MLE$par
se
par <- c(Intercept = 0.13983,x = 2.72655, sigma2 = -1.43907)
se <- c(Intercept = 0.25141, x = 0.04136, sigma2 = 0.07191)
comb <- paste0(par, "\n(", se, ")")
table(Parameter = names(par),
Estimate = comb,
row.names = NA)
table(Parameter = names(par),
Estimate = comb,
row.names = NULL)
par <- c(Intercept = 0.13983,x = 2.72655, sigma2 = -1.43907)
se <- c(Intercept = 0.25141, x = 0.04136, sigma2 = 0.07191)
comb <- paste0(par, "\n(", se, ")")
table(Parameter = names(par),
Estimate = comb,
row.names = NULL)
comb <- paste0(par, "\n(", se, ")")
comb
par <- c(0.13983, 2.72655,-1.43907)
se <- c(0.25141, 0.04136, 0.07191)
names(par) <- c("Intercept", "x", "Sigma2")
comb <- paste0(par, "\n(", se, ")")
comb
table(Parameter = names(par),
Estimate = comb,
row.names = NULL)
length(names(par))
length(comb)
linear_MLE_output <- data.frame(Parameter = names(par),
Estimate = comb,
row.names = NULL)
xtable::xtable(linear_MLE_output)
linear_MLE_output <- data.frame(Parameter = names(par),
Estimate = comb,
stringsAsFactors = FALSE)
xtable::xtable(linear_MLE_output)
#####################
# load libraries
# set wd
# clear global .envir
#####################
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("stargazer", "broom"),  pkgTest)
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#####################
# Problem 1
#####################
ks_test <- function(input, tol = 1e-12) {
ECDF <- ecdf(input)
empiricalCDF <- ECDF(input)
D <- max(abs(empiricalCDF - pnorm(input)))
n <- length(input)
if (D <= 0) p <- 1 # if D = 0 cdfs are in perfect agreement
else if (D >= 1) p <- 0 #max deviation
else {
x <- sqrt(n) * D #scaled test statistic
if (x > 1.94) {
c <- 2.000021 + 0.331 / sqrt(n) + 1.409 / n
p <- (2 * exp(-c * x^2))
} else {
p <- 0 #initialise series at 0
i <- 1
repeat {
term <- (2 * (-1)^(i - 1) * exp( -2 * i^2 * x^2)) #Limiting form formula from MWT
p <- p + term #loop over i's  and sum
if (abs(term) < tol) break
i <- i + 1
}
}
}
p <- max(min(p,1), 0)
results <- list("Test Statistic D" = D, "P-Value" = p)
return(results)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test(data)
ks.test(data, "pnorm")
#####################
# Problem 2
#####################
set.seed (123)
data <- data.frame(x = runif(200, 1, 10))
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5)
# Plot and visualise the distribution
library(ggplot2)
ggplot(data, aes(x = y)) + geom_density()
ggplot(data, aes(x = x, y = y)) + geom_point()
##  From Scratch - Create linear log likelihood function ##
linear_lik <- function(y, X, theta) { #output, input, theta = vector of unknown parms
n      <- nrow(X) #size of sample
k      <- ncol(X) # number of parameters to estimate
beta   <- theta[1:k] # vector of coefficients
sigma2 <- theta[k+1]^2 # variance
e      <- y - X%*%beta #errors with mean of Xbeta (as normal distribution)
logl   <- -.5*n*log(2*pi)-.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return(-logl) #as optim function finds min reverse log likelihood
}
# Run MLE using linear likelihood and data
linear_MLE <- optim(fn=linear_lik,  #using scratch function
y =data$y,  #outcome var
X =cbind(1, data$x),  #input var plus 1's for intercept
par=c(1,1,1),  #parameter starting points
hessian=TRUE, #find standard errors
method="BFGS")
linear_MLE$par
#Find Standard Errors
H <- linear_MLE$hessian
vcov <- solve(H)
se <-sqrt(diag(vcov))
se
par <- c(0.13983, 2.72655,-1.43907) #parameters from model
se <- c(0.25141, 0.04136, 0.07191) #se's
names(par) <- c("Intercept", "x", "Sigma2")
comb <- paste0(par, "\n(", se, ")") #combined in format usual regression
comb
linear_MLE_output <- data.frame(Parameter = names(par), #make table
Estimate = comb,
stringsAsFactors = FALSE)
xtable::xtable(linear_MLE_output)
## OLS LM
LM <- lm(data$y~data$x)
stargazer(LM,
type = "latex",
title = "OLS Linear Regession lm")
# GLS Gaussian
GLM_gaussian <- glm(data$y~data$x, family = "gaussian")
stargazer(GLM_gaussian,
type = "latex",
title = "GLM gaussian")
# GLS Gaussian
GLM_gaussian <- glm(data$y~data$x, family = "gaussian", link = "identity")
vcov <- solve(-H) #compute inverse matrix H^-1
se <-sqrt(diag(vcov))
vcov <- solve(H) #compute inverse matrix H^-1
View(linear_MLE)
H
#Find Standard Errors
H <- linear_MLE$hessian #extract the hessian matrix
cov_matrix <- solve(H) #compute inverse matrix H^-1 (hessian for neg. log likelihood so just take inverse)
se <-sqrt(diag(cov_matrix)) #taking the square root of the covariance matrix for ses
se
se
ks_test <- function(input, tol = 1e-12) {
ECDF <- ecdf(input)
empiricalCDF <- ECDF(input)
D <- max(abs(empiricalCDF - pnorm(input))) #test statistic
n <- length(input) #sample size
if (D <= 0) p <- 1 # if D = 0 cdfs are in perfect agreement
else if (D >= 1) p <- 0 #max deviation
else {
x <- sqrt(n) * D #scaled test statistic
if (x > 1.94) { #condition from MSW
c <- 2.000021 + 0.331 / sqrt(n) + 1.409 / n #extra large x approximation
p <- (2 * exp(-c * x^2))
} else {
p <- 0 #initialise series at 0
i <- 1
repeat {
term <- (2 * (-1)^(i - 1) * exp( -2 * i^2 * x^2)) #Limiting form formula from MWT
p <- p + term #loop over i's  and sum
if (abs(term) < tol) break
i <- i + 1
}
}
}
p <- max(min(p,1), 0)
results <- list("Test Statistic D" = D, "P-Value" = p)
return(results)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test(data)
}
ks_test <- function(input, tol = 1e-12) {
ECDF <- ecdf(input)
empiricalCDF <- ECDF(input)
D <- max(abs(empiricalCDF - pnorm(input))) #test statistic
n <- length(input) #sample size
if (D <= 0) p <- 1 # if D = 0 cdfs are in perfect agreement
else if (D >= 1) p <- 0 #max deviation
else {
x <- sqrt(n) * D #scaled test statistic
#if (x > 1.94) { #condition from MSW
#c <- 2.000021 + 0.331 / sqrt(n) + 1.409 / n #extra large x approximation
#p <- (2 * exp(-c * x^2)) }
else {
ks_test <- function(input, tol = 1e-12) {
ECDF <- ecdf(input)
empiricalCDF <- ECDF(input)
D <- max(abs(empiricalCDF - pnorm(input))) #test statistic
n <- length(input) #sample size
if (D <= 0) p <- 1 # if D = 0 cdfs are in perfect agreement
else if (D >= 1) p <- 0 #max deviation
else {
x <- sqrt(n) * D #scaled test statistic
#if (x > 1.94) { #condition from MSW
#c <- 2.000021 + 0.331 / sqrt(n) + 1.409 / n #extra large x approximation
#p <- (2 * exp(-c * x^2)) }
#else {
p <- 0 #initialise series at 0
i <- 1
repeat {
term <- (2 * (-1)^(i - 1) * exp( -2 * i^2 * x^2)) #Limiting form formula from MWT
p <- p + term #loop over i's  and sum
if (abs(term) < tol) break
i <- i + 1
#}
}
}
p <- max(min(p,1), 0)
results <- list("Test Statistic D" = D, "P-Value" = p)
return(results)
}
set.seed(123)
data <- rcauchy(1000, location = 0, scale = 1)
ks_test(data)
