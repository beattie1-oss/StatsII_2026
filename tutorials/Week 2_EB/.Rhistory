#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-l, lower.tail = F)
}
lm_by_hand <- function(inputDF, covariates, outcome) {
#create matrices
X <- as.matrix(cbind(rep(1,dim(inputDF)[1]), inputDF[ , covariates]))
Y <- reg_DF[ , outcome]
#calculate betas
betas <- solve((t(X)%*%X)) %*% (t(X)%*%Y)
rownames(betas[1]) <- "Intercept"
n <- dim(inputDF)[1]
k <- ncol(X)
#calculate SE for betas
#estimate if sigma-squared
sigma_squared <- sum((Y - X%*%betas)^2) / (nrow(X) - ncol(X))
#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-l, lower.tail = F)
}
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
lm_by_hand <- function(inputDF, covariates, outcome) {
#create matrices
X <- as.matrix(cbind(rep(1,dim(inputDF)[1]), inputDF[ , covariates]))
Y <- reg_DF[ , outcome]
#calculate betas
betas <- solve((t(X)%*%X)) %*% (t(X)%*%Y)
rownames(betas)[1] <- "Intercept"
n <- dim(inputDF)[1]
k <- ncol(X)
#calculate SE for betas
#estimate if sigma-squared
sigma_squared <- sum((Y - X%*%betas)^2) / (nrow(X) - ncol(X))
#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-l, lower.tail = F)
}
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
lm_by_hand <- function(inputDF, covariates, outcome) {
#create matrices
X <- as.matrix(cbind(rep(1,dim(inputDF)[1]), inputDF[ , covariates]))
Y <- reg_DF[ , outcome]
#calculate betas
betas <- solve((t(X)%*%X)) %*% (t(X)%*%Y)
rownames(betas)[1] <- "Intercept"
n <- dim(inputDF)[1]
k <- ncol(X)
#calculate SE for betas
#estimate if sigma-squared
sigma_squared <- sum((Y - X%*%betas)^2) / (nrow(X) - ncol(X))
#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-k, lower.tail = F)
}
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
View(reg_results)
print reg_results
print(reg_results)
print(auto_results)
auto_results <- lm(bushiraq ~ ., data =reg_DF)
print(auto_results)
print(reg_results)
summary(reg_results)
load("/Users/ellen/Statistical Analysis/datasets/anes.Rdata")
anes <- anes[complete.cases(anes$caseid),]
reg_DF <- anes[,c("white", "female", "age", "partyid", "bushiraq")]
lm_by_hand <- function(inputDF, covariates, outcome) {
#create matrices
X <- as.matrix(cbind(rep(1,dim(inputDF)[1]), inputDF[ , covariates]))
Y <- input_DF[ , outcome]
#calculate betas
betas <- solve((t(X)%*%X)) %*% (t(X)%*%Y)
rownames(betas)[1] <- "Intercept"
n <- dim(inputDF)[1]
k <- ncol(X)
#calculate SE for betas
#estimate if sigma-squared
sigma_squared <- sum((Y - X%*%betas)^2) / (nrow(X) - ncol(X))
#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-k, lower.tail = F)
}
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
lm_by_hand <- function(inputDF, covariates, outcome) {
#create matrices
X <- as.matrix(cbind(rep(1,dim(inputDF)[1]), inputDF[ , covariates]))
Y <- inputDF[ , outcome]
#calculate betas
betas <- solve((t(X)%*%X)) %*% (t(X)%*%Y)
rownames(betas)[1] <- "Intercept"
n <- dim(inputDF)[1]
k <- ncol(X)
#calculate SE for betas
#estimate if sigma-squared
sigma_squared <- sum((Y - X%*%betas)^2) / (nrow(X) - ncol(X))
#create variance-covariance matrix for betas
var_covar_mat <- sigma_squared*solve(t(X)%*%X)
#standard errors for coefficient estimates
SEs <- sqrt(diag(var_covar_mat))
#get t-stat and p-values
TS <- (betas - 0) / SEs
p_values <- 2*pt(abs(TS), n-k, lower.tail = F)
}
reg_results <- lm_by_hand(reg_DF, c("white", "female", "age", "partyid"), "bushiraq")
View(reg_DF)
print(reg_results)
#Q2
#a) Hypothesis Test assigned to lawn signs
ts_1 <- 0.042 / 0.016
p_1 <- 2*pt(abs(ts_1), 128, lower.tail = FALSE)
#b) Hypothesis test adjacent to lawn signs
ts_2 <- 0.042 / 0.013
p_2 <- 2*pt(abs(ts_2), 128, lower.tail = FALSE)
p_2
F.test <- (0.094/ (2-1)) / ((1-0.094)/(131-2))
pf(F.test,1,129, lower.tail = FALSE)
p_f <- pf(F.test,1,129, lower.tail = FALSE)
print(ts_1, p_1)
ts_1 <- 0.042 / 0.016
ts_1
p_1 <- 2*pt(abs(ts_1), 128, lower.tail = FALSE)
p_1
ts_1, p_1
#Q2 - 65
#a) Hypothesis Test assigned to lawn signs
ts_1 <- 0.042 / 0.016
p_1 <- 2*pt(abs(ts_1), 128, lower.tail = FALSE)
ts_1
p_1
#Q2b) Hypothesis test adjacent to lawn signs
ts_2 <- 0.042 / 0.013
ts_2
p_2 <- 2*pt(abs(ts_2), 128, lower.tail = FALSE)
p_2
F.test <- (0.094/ (2)) / ((1-0.094)/(131-2-1))
F.test <- (0.094/ 2) / ((1-0.094)/(131-2-1))
p_f <- pf(F.test,1,129, lower.tail = FALSE)
p_f
p_f <- pf(F.test, 2, 129, lower.tail = FALSE)
p_f
p_f <- pf(F.test, 2, 128, lower.tail = FALSE)
p_f
1- pf(6.640, 2, 128)
p_f <- pf(F.test, 2, 128, lower.tail = FALSE)
p_f <- 1- pf(6.640, 2, 128)
p_f <- pf(F.test, 2, 128, lower.tail = FALSE)
p_f2 <- 1- pf(6.640, 2, 128)
p_f
#d) Evaluate model fit f-test
F.test <- (0.094/2) / ((1-0.094)/(131-2-1))
F.test
p_f <- pf(F.test, 2, 128, lower.tail = FALSE)
p_f
?t.test
f <- (0.094/1) / ((1-0.094)/(131-2))
f <- (0.515/4) / ((1-0.515)/(378))
f <- (0.515/3) / ((1-0.515)/(378))
f <- (0.515/4) / ((1-0.515)/(378))
f <- (0.5154188747/4) / ((1-0.515)/(378))
#d) Evaluate model fit f-test
F.test <- (0.094/2) / ((1-0.094)/(131-2-1))
f <- (0.515/3) / ((1-0.515)/(379))
f <- (0.515/4) / ((1-0.515)/(378))
install.packages(my_packages, repos = "http://cran.rstudio.com")
my_packages <- c("tidyverse", "broom", "coefplot", "cowplot",
"gapminder", "GGally", "ggrepel", "ggridges", "gridExtra",
"here", "interplot", "margins", "maps", "mapproj",
"mapdata", "MASS", "quantreg", "rlang", "scales",
"survey", "srvyr", "viridis", "viridisLite", "devtools")
getwd()
### Set UP ###
rm(list=ls())
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
iris
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
lm(data = Iris, Sepal.Length ~ Sepal.Width + Species)
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
load(Iris)
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
data("iris")
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
data(Iris)
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
Iris
# Logic: use maximum likelihood estimation, take derivatives and try to minimise it, be confident that it is the best combination of parameters t
# this then helps us find the best relationship
# lm() is special type of glm() => both logic estimation approach arrives at the best set of parameters
iris
lm(data = iris, Sepal.Length ~ Sepal.Width + Species)
glm(ata = iris, Sepal.Length ~ Sepal.Width + Species)
lm(data = iris, Sepal.Length ~ Sepal.Width + Species) #linear
glm(ata = iris, Sepal.Length ~ Sepal.Width + Species, family = "gaussian")
glm(data = iris, Sepal.Length ~ Sepal.Width + Species, family = "gaussian")
## loading the data
data <-
## loading the data
data <-
# If all Values = 0 perfect model
# CLose to 0 much of variation explained by model
# Higher deviance = Bad
# Do not draw conclusions, try new model, use these values to compare
#There is no threshold, it is a comparitive (the size depends on units)
# AIC/BIC penalty term increasing model complexity (new terms) simple is better. Is predictive power improving?
# Log likelihood value -
# Deviance = how much info you failed to capture
#Logit regression. Use log odds to convert binomial, as it is a infity scale which is a straight line
# Log of odds also converts model from multiplicative to an additive relationships
0.9 / 0.1
# If all Values = 0 perfect model
# CLose to 0 much of variation explained by model
# Higher deviance = Bad
# Do not draw conclusions, try new model, use these values to compare
#There is no threshold, it is a comparitive (the size depends on units)
# AIC/BIC penalty term increasing model complexity (new terms) simple is better. Is predictive power improving?
# Log likelihood value -
# Deviance = how much info you failed to capture
#Logit regression. Use log odds to convert binomial, as it is a infity scale which is a straight line
# Log of odds also converts model from multiplicative to an additive relationships
log(0.9 / 0.1)
log(0.1/0.9)
data$set <- ifelse(iris$Species == "setosa", 1, 0)
mod1 <- lm(set ~ Petal.Length, + Petal.Width, data = dat)
mod1 <- lm(set ~ Petal.Length, + Petal.Width, data = data)
mod1 <- lm(data = iris, set ~ Petal.Length, + Petal.Width)
#Using Logistic regression
dat<- iris
dat$set <- ifelse(iris$Species == "setosa", 1, 0)
mod1 <- lm(data = dat, set ~ Petal.Length, + Petal.Width)
print(mod1)
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# Load any necessary packages
lapply(c("tidyverse", "ggplot2"),  pkgTest)
# Set working directory for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Remove objects
rm(list=ls())
# Detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# Load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# Load any necessary packages
lapply(c("tidyverse", "ggplot2"),  pkgTest)
# Set working directory for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Set working directory for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Load any necessary packages
lapply(c("tidyverse", "ggplot2","readr"),  pkgTest)
## loading the data
data <- read_csv("~/GitHub/StatsII_2026/tutorials/Week 2_EB/cbg_turnover_v23upload.xlsx")
View(data)
## loading the data
data <- read_excel('mep_info_26Jul11.xls', sheet = "`data v2023`")
# Load any necessary packages
lapply(c("tidyverse", "ggplot2","readxl"),  pkgTest)
## loading the data
data <- read_excel('mep_info_26Jul11.xls', sheet = "data v2023")
## loading the data
data <- read_excel("cbg_turnover_v23upload.xlsx", sheet = "data v2023")
View(data)
str(data)
str(data_raw)
## loading the data
data_raw <- read_excel("cbg_turnover_v23upload.xlsx", sheet = "data v2023")
str(data_raw)
#### Wrangling the data
# We should now have a dataset where our variables are at least of the correct type
# However, we need to do a bit of tidying to get the data into a more user-friendly
# format.
data <- data_raw %>%
select(codewdi,
country,
year,
`time to regular turnover`,
`regular turnover dummy`,
`irregular turnover dummy`,
`legal duration`
) %>%
mutate(
codewdi = as.factor(codewdi),
country = as.factor(country),
year = as.integer(year),
time_to_regular_turnover = as.integer(`time to regular turnover`),
regular+turnover_dummy  = as.integer(`regular turnover dummy`),
data <- data_raw %>%
select(codewdi,
country,
year,
`time to regular turnover`,
`regular turnover dummy`,
`irregular turnover dummy`,
`legal duration`
) %>%
mutate(
codewdi = as.factor(codewdi),
country = as.factor(country),
year = as.integer(year),
time_to_regular_turnover = as.integer(`time to regular turnover`),
regular_turnover_dummy  = as.integer(`regular turnover dummy`),
irregular_turnover_dummy  = as.integer(`irregular turnover dummy`),
legal_duration  = as.integer(`legal duration`),
) %>%
drop_na()
data <- data %>%
select(
codewdi,
country,
year,
time_to_regular_turnover,
regular_turnover_dummy,
irregular_turnover_dummy,
legal_duration
)
unique(data$codewdi)
unique(data$codewdi)
unique(data$country)
unique(data$year)
unique(data$time_to_regular_turnover)
unique(data$regular_turnover_dummy)
unique(data$irregular_turnover_dummy)
unique(data$legal_duration)
bad_codes <- c(-999, -666, -535, -881)
data <- data %>%
mutate %>%
across(
c(year, time_to_regular_turnover, regular_turnover_dummy, irregular_turnover_dummy, legal_duration
),
~ replace(., . %>% bad_codes, NA)
)
data <- data %>%
mutate(
across(
c(year, time_to_regular_turnover, regular_turnover_dummy, irregular_turnover_dummy, legal_duration
),
~ replace(., . %>% bad_codes, NA)
)
)
data <- data %>%
mutate(
across(
c(year, time_to_regular_turnover, regular_turnover_dummy, irregular_turnover_dummy, legal_duration
),
~ replace(., . %in% bad_codes, NA)
)
)
unique(data$codewdi)
unique(data$country) # 163
unique(data$year) #60ish
unique(data$time_to_regular_turnover) #get
unique(data$regular_turnover_dummy)
unique(data$irregular_turnover_dummy)
unique(data$legal_duration)
bad_codes <- c(-999, -666, -555, -535, -881)
data <- data %>%
mutate(
across(
c(year, time_to_regular_turnover, regular_turnover_dummy, irregular_turnover_dummy, legal_duration
),
~ replace(., . %in% bad_codes, NA)
)
)
#Check turned to NA
unique(data$codewdi)
unique(data$country) # 163
unique(data$year) #60ish
unique(data$time_to_regular_turnover) #get
unique(data$regular_turnover_dummy)
unique(data$irregular_turnover_dummy)
unique(data$legal_duration)
data %>% drop_na()
country_turnouver <- data %>%
group_by(country) %>%
summarize(
avg_turnover = mean(irregular_turnover_dummy),
n = n()
)
# (a) Which five countries have the highest average turnover rates?
country_turnouver %>%
arrange(desc(avg_turnover)) %>%
slice[1:5]
# (a) Which five countries have the highest average turnover rates?
country_turnouver %>%
arrange(desc(avg_turnover)) %>%
slice(1:5)
country_turnover <- data %>%
group_by(country) %>%
summarize(
avg_turnover = mean(irregular_turnover_dummy),
n = n()
)
# (a) Which five countries have the highest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(1:5) #recieve first five
# (b) Which five have the lowest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(-1:5) #recieve first five
# (b) Which five have the lowest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(-1:-5) #recieve first five
country_turnover <- data %>%
group_by(country) %>%
summarize(
avg_turnover = mean(irregular_turnover_dummy), #average irregular turnover , likelihood that governer gets turned over
n = n()
)
# (a) Which five countries have the highest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(1:5) #recieve first five
# (a) Which five countries have the highest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(1:5) #recieve first five
# (a) Which five countries have the highest average turnover rates?
country_turnover %>%
arrange(desc(avg_turnover)) %>%
slice(1:5) #recieve first five
# (b) Which five have the lowest average turnover rates?
country_turnover %>%
arrange(incr(avg_turnover)) %>%
slice(1:5) #recieve first five
slice(1:5) #recieve first five
# (b) Which five have the lowest average turnover rates?
country_turnover %>%
arrange(avg_turnover) %>%
slice(1:5) #recieve first five
# (a) Fit lm() with:
# Outcome: irregular turnover dummy
# Covariates:
#   time to regular turnover
#   legal duration
#If time to turnover is less, the previos govt spent some time
# Higher time to turnover, recently saw a change in governer
lmp <-
lm(irregular_turnover_dummy ~ time_to_regular_turnover + legal duration , data = data)
# (a) Fit lm() with:
# Outcome: irregular turnover dummy
# Covariates:
#   time to regular turnover
#   legal duration
#If time to turnover is less, the previos govt spent some time
# Higher time to turnover, recently saw a change in governer
lmp <-
lm(irregular_turnover_dummy ~ time_to_regular_turnover + legal_duration , data = data)
summary(lmp)
# Estimate a logistic regression with governor turnover as the binary outcome and same covariates using glm(family = "binomial")
logit <- glm(
irregular_turnover_dummy ~ time_to_regular_turnover + legal_duration , data = data, family = "binomial"
)
summary(logit)
